# mpb-ml
Este repositÃ³rio contÃ©m o Modelo de Aprendizado de MÃ¡quina, Conjunto de Dados e CÃ³digo de Treinamento para AnÃ¡lise de ViÃ©s PolÃ­tico em VeÃ­culos de ComunicaÃ§Ã£o Brasileiros
<br><br>
# ğŸš€ InstalaÃ§Ã£o
## Clone o repositÃ³rio
```bash
git clone https://github.com/renatocecchetti/mpb-ml.git
cd mpb-ml
```

## Instale as dependÃªncias
```bash
pip install -r requirements.txt
```
<br><br>
# Principais Recrusos

## News Portal Scraper
Sistema de coleta automatizada de notÃ­cias dos principais portais jornalÃ­sticos do Brasil.

### ğŸ“° Sobre
Sistema que realiza scraping de colunas polÃ­ticas dos seguintes portais:

- G1
- CNN Brasil
- Folha de SÃ£o Paulo
- Gazeta do Povo
- IstoÃ‰
- MetrÃ³poles

### ğŸ’» Como Usar
#### Uso BÃ¡sico
```python
from NewsPortalScraper import NewsPortalScraper

# Inicializa o scraper
scraper = NewsPortalScraper()

# Coleta de um portal especÃ­fico
g1_texts = scraper.scrape_g1(limit_per_columnist=100)
scraper.save_portal_texts('g1', g1_texts, 'g1_political_news.txt')

# Ou coleta de todos os portais
all_texts = scraper.scrape_all(limit_per_columnist=100)
for portal, texts in all_texts.items():
    scraper.save_portal_texts(portal, texts, f'{portal}_political_news.txt')
```
#### MÃ©todos DisponÃ­veis
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `scrape_g1(limit_per_columnist=100)` | Coleta textos dos colunistas polÃ­ticos do G1 |
| `scrape_cnn(limit_pages=10)` | Coleta textos dos colunistas da CNN Brasil |
| `scrape_folha(limit_per_columnist=100)` | Coleta textos dos colunistas da Folha |
| `scrape_gazeta(limit_per_columnist=20)` | Coleta textos dos colunistas da Gazeta |
| `scrape_istoe(limit_per_columnist=100)` | Coleta textos dos colunistas da IstoÃ‰ |
| `scrape_metropoles(limit_per_columnist=20)` | Coleta textos dos colunistas do MetrÃ³poles |
| `scrape_all(limit_per_columnist=100)` | Coleta textos de todos os portais |
| `save_portal_texts(portal, texts, filename)` | Salva os textos coletados em arquivo |

#### Exemplo de Coleta EspecÃ­fica
```python
# Coleta apenas da CNN Brasil
cnn_texts = scraper.scrape_cnn(limit_pages=10)
scraper.save_portal_texts('cnn', cnn_texts, 'cnn_political_news.txt')

# Coleta apenas da Folha com limite personalizado
folha_texts = scraper.scrape_folha(limit_per_columnist=50)
scraper.save_portal_texts('folha', folha_texts, 'folha_political_news.txt')
```
#### ğŸ“‹ Requisitos
```bash
Python 3.7+
requests>=2.31.0
beautifulsoup4>=4.12.2
tqdm>=4.66.1
lxml>=4.9.3
```
### âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes
- O scraper respeita delays entre requisiÃ§Ãµes para evitar sobrecarga dos servidores
- Alguns portais podem requerer autenticaÃ§Ã£o para acesso completo ao conteÃºdo
- As classes CSS dos portais podem mudar, necessitando atualizaÃ§Ã£o do cÃ³digo
- O nÃºmero real de textos coletados pode ser menor que o limite definido

## ğŸ›ï¸ Coletor de Discursos de Deputados

### Sobre
Classe Python para coletar discursos de deputados atravÃ©s da API da CÃ¢mara dos Deputados e processo de enriquecimento com dados de espectro polÃ­tico dos partidos brasileiros

### ğŸš€ Como Usar

#### Uso BÃ¡sico
```python
from DiscursosDeputadosCollector import DiscursosDeputadosCollector
from PoliticalSpectrumEnricher import PoliticalSpectrumEnricher

# Exemplo de uso
collector = DiscursosDeputadosCollector()

path = '../../data/speech'
speech_file = f'{path}/Discursos.csv'
party_file = f'{path}/Partidos.csv'
merged_file = f'{path}/Discursos_Enriquecidos.csv'

# Coleta discursos de um perÃ­odo especÃ­fico
df = collector.collect_discursos(
    data_inicio='2025-02-01',
    data_fim='2025-02-05',
    output_file=speech_file
)

# AnÃ¡lise dos dados
print(f"Total de discursos coletados: {len(df)}")
print("\nAmostra de transcricÃµes:")
print(df.sample(5)['transcricao'])

# Exemplo de uso
enricher = PoliticalSpectrumEnricher()

# Carrega os dados
enricher.load_data(
    partidos_path=party_file,
    discursos_path=speech_file
)

# Enriquece os dados
df_enriched = enricher.enrich_data()

# Salva os dados enriquecidos
enricher.save_enriched_data(merged_file)

# ObtÃ©m estatÃ­sticas
stats = enricher.get_spectrum_statistics()
print("\nEstatÃ­sticas por Espectro PolÃ­tico:")
print(stats)
```
### ğŸ“Š Estrutura dos Dados Coletados e Enriquecidos

| Coluna | DescriÃ§Ã£o |
|:-------|:----------|
| `email` | Email institucional do deputado |
| `id` | ID Ãºnico do deputado na CÃ¢mara |
| `idLegislatura` | ID da legislatura atual |
| `nome` | Nome completo do parlamentar |
| `siglaPartido` | Sigla do partido polÃ­tico |
| `siglaUf` | Unidade federativa que representa |
| `uri` | URI do deputado na API |
| `uriPartido` | URI do partido na API |
| `urlFoto` | URL da foto oficial |
| `dataHoraFim` | Timestamp do fim do discurso |
| `dataHoraInicio` | Timestamp do inÃ­cio do discurso |
| `keywords` | Palavras-chave do discurso |
| `sumario` | Resumo do conteÃºdo |
| `tipoDiscurso` | ClassificaÃ§Ã£o do discurso |
| `transcricao` | Texto completo |
| `urlAudio` | Link para o Ã¡udio |
| `urlVideo` | Link para o vÃ­deo |
| `Espectro PolÃ­tico` | Espectro PolÃ­tico do Partido |

### ğŸ“‹ Requisitos
```bash
Python 3.7+
requests>=2.31.0
pandas>=2.0.0
python-dateutil>=2.8.2
```
### âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes
- A API pode ter limites de requisiÃ§Ãµes
- Alguns discursos podem nÃ£o ter transcriÃ§Ã£o disponÃ­vel
- O tempo de coleta pode variar dependendo do perÃ­odo solicitado
- Necessita conexÃ£o estÃ¡vel com a internet
<br><br>
# ğŸ—ï¸ Estrutura do Projeto
```tree
news-portal-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news_scraper.py
â”‚   â””â”€â”€ news_portal_scraper.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
<br><br>
# ğŸ“ LicenÃ§a
Este projeto estÃ¡ licenciado sob a licenÃ§a Apache-2.0 license - veja o arquivo [LICENSE](http://www.apache.org/licenses/LICENSE-2.0) para detalhes.
<br><br>

# ğŸ¤ Contribuindo
ContribuiÃ§Ãµes sÃ£o bem-vindas!