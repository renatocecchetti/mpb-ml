# mpb-ml - Media Political Bias Machine Learning
Este repositÃ³rio contÃ©m o Modelo de Aprendizado de MÃ¡quina, Conjunto de Dados e CÃ³digo de Treinamento para AnÃ¡lise de ViÃ©s PolÃ­tico em VeÃ­culos de ComunicaÃ§Ã£o Brasileiros

Pode ser utilizado de forma completa atravÃ©s da execuÃ§Ã£o do pipeline ou de forma modularizada realizando a chamada Ã  cada uma das funÃ§Ãµes. 
<br><br>
# ğŸš€ InstalaÃ§Ã£o
## Clone o repositÃ³rio
```bash
git clone https://github.com/renatocecchetti/mpb-ml.git
cd mpb-ml
```

## Instale as dependÃªncias
```bash
pip install -r requirements.txt
```
<br><br>

# Pipeline Completo
O pipeline irÃ¡ realizar os seguintes passos:
1. Coleta dos discursos dos deputados
2. Enriquecimento com dados do espectro polÃ­tico dos partidos
3. Coleta de textos das colunas dos portais
4. Treinamento do modelo de classificaÃ§Ã£o de viÃ©s
5. RealizaÃ§Ã£o de inferÃªncia nos dados dos portais

## ğŸ’» Como Usar
```bash
python src/main.py
```
## VisualizaÃ§Ã£o dos Resultados
Um exemplo de visualizaÃ§Ã£o do resultado do Pipeline Completo encontra-se disponÃ­vel no Jupyter Notebook [MediaBiasReport.ipynb](https://github.com/renatocecchetti/mpb-ml/blob/main/notebooks/MediaBiasReport.ipynb)

# Arquivo de ConfiguraÃ§Ã£o (config.yaml)
## VisÃ£o Geral
O arquivo config.yaml centraliza todas as configuraÃ§Ãµes do sistema de anÃ¡lise de viÃ©s polÃ­tico em texto. Este guia detalha cada seÃ§Ã£o e suas opÃ§Ãµes.

Estrutura do Arquivo
1. ConfiguraÃ§Ãµes Gerais (general)
```bash
general:
  data_dir: 'data'              # DiretÃ³rio base para dados
  data_dir_portals: 'data/portals'  # SubdiretÃ³rio para dados dos portais
  data_dir_speech: 'data/speech'    # SubdiretÃ³rio para discursos
  models_dir: 'models'          # DiretÃ³rio para modelos treinados
  output_dir: 'output'          # DiretÃ³rio para resultados
  log_level: 'INFO'             # NÃ­vel de logging
  log_format: '%(asctime)s...'  # Formato das mensagens de log

```
2. ConfiguraÃ§Ãµes do Modelo (model)
```bash
model:
  name: 'political_bias_model.joblib'  # Nome do arquivo do modelo
  bert_model: 'neuralmind/bert-base-portuguese-cased'  # Modelo BERT prÃ©-treinado
  hidden_layer_sizes: [100]     # Arquitetura da rede neural
  max_iter: 5000                # MÃ¡ximo de iteraÃ§Ãµes
  random_state: 1               # Semente aleatÃ³ria
  embeddings_file_name: 'embeddings.npy'  # Arquivo de embeddings
  reuse_embedding: False        # Reutilizar embeddings existentes
```
3. Portais de NotÃ­cias (news_portals)
- supported_portals: Lista de portais suportados
- Para cada portal:
    - columnists: DicionÃ¡rio de colunistas e suas URLs
    - content_class: Classe CSS para extrair conteÃºdo
    - post_class: Classe CSS para identificar posts

Exemplo de configuraÃ§Ã£o de portal:
```bash
g1:
  columnists:
    andreia_sadi: 'https://g1.globo.com/politica/blog/andreia-sadi/'
    # ...
  content_class: 'mc-column content-text active-extra-styles'
  post_class: 'bastian-feed-item'
```
4. ConfiguraÃ§Ãµes de Scraping (scraping)
```bash
scraping:
  user_agent: 'Mozilla/5.0...'  # User agent para requisiÃ§Ãµes
  timeout: 10                   # Timeout em segundos
  sleep_time: 0.5              # Intervalo entre requisiÃ§Ãµes
  max_retries: 3               # MÃ¡ximo de tentativas
  items_per_page: 100          # Itens por pÃ¡gina
  limit_per_columnist: 100     # Limite de artigos por colunista
```
5. VisualizaÃ§Ã£o (visualization)
```bash
visualization:
  figure_size: [10, 6]         # Tamanho dos grÃ¡ficos
  colors:                      # Cores por orientaÃ§Ã£o polÃ­tica
    left: 'red'
    center: 'gray'
    right: 'blue'
  spectrum_order: ['Esquerda', 'Centro', 'Direita']  # Ordem no grÃ¡fico
```
6. API da CÃ¢mara (camara_api)
```bash
camara_api:
  base_url: 'https://dadosabertos.camara.leg.br/api/v2'
  endpoints:                    # Endpoints da API
    deputados: '/deputados'
    discursos: '/deputados/{id}/discursos'
  params:                      # ParÃ¢metros padrÃ£o
    ordem: 'ASC'
    ordenarPor: 'nome'
    itens_por_pagina: 100
```
7. ConfiguraÃ§Ãµes de Discursos (discursos)
```bash
discursos:
  paths:                       # Caminhos dos arquivos
    base_dir: 'data/speech'
    discursos_file: 'Discursos.csv'
    # ...
  data_collection:            # ParÃ¢metros de coleta
    data_inicio: '2021-03-02'
    data_fim: '2025-03-01'
    sample_size: 5
  required_columns:           # Colunas obrigatÃ³rias
    discursos: ['transcricao', 'siglaPartido', ...]
    partidos: ['Sigla', 'Nome', ...]
  spectrum_mapping:          # Mapeamento do espectro polÃ­tico
    'Centro': 'Centro'
    'Centro-direita': 'Direita'
    # ...
```
### Uso e ManutenÃ§Ã£o
#### Adicionando Novos Portais
1. Adicione o nome do portal em supported_portals
2. Crie uma nova seÃ§Ã£o com:
    - Lista de colunistas
    - Classes CSS necessÃ¡rias
    - ConfiguraÃ§Ãµes especÃ­ficas

#### Atualizando ConfiguraÃ§Ãµes
- Scraping: Ajuste sleep_time e timeout conforme necessÃ¡rio
- Modelo: Modifique parÃ¢metros do modelo em model
- VisualizaÃ§Ã£o: Personalize cores e tamanhos em visualization

#### ManutenÃ§Ã£o
- Verifique URLs periodicamente
- Atualize classes CSS quando os sites mudarem
- Ajuste parÃ¢metros de scraping conforme necessidade

#### Boas PrÃ¡ticas
- Mantenha backup do arquivo
- Documente alteraÃ§Ãµes
- Teste novas configuraÃ§Ãµes em ambiente de desenvolvimento
- Monitore logs para ajustes de parÃ¢metros

#### ObservaÃ§Ãµes
- Classes CSS podem mudar com atualizaÃ§Ãµes dos portais
- Respeite limites de requisiÃ§Ãµes dos sites
- Mantenha sleep_time adequado para evitar bloqueios
- FaÃ§a backup regular dos dados coletados

<br>
Este arquivo de configuraÃ§Ã£o centraliza todas as configuraÃ§Ãµes do sistema, facilitando manutenÃ§Ã£o e ajustes sem necessidade de alterar o cÃ³digo-fonte.

<br>

# Modularizado
## Scrapper de portais de notÃ­cias
Sistema de coleta automatizada de notÃ­cias dos principais portais jornalÃ­sticos do Brasil.

### ğŸ“° Sobre
Sistema que realiza scraping de colunas polÃ­ticas dos seguintes portais:

- G1
- CNN Brasil
- Folha de SÃ£o Paulo
- Gazeta do Povo
- IstoÃ‰
- MetrÃ³poles

### ğŸ’» Como Usar
#### Uso BÃ¡sico
```python
from NewsPortalScraper import NewsPortalScraper

# Inicializa o scraper
scraper = NewsPortalScraper()

# Coleta de um portal especÃ­fico
g1_texts = scraper.scrape_g1(limit_per_columnist=100)
scraper.save_portal_texts('g1', g1_texts, 'g1_political_news.txt')

# Ou coleta de todos os portais
all_texts = scraper.scrape_all(limit_per_columnist=100)
for portal, texts in all_texts.items():
    scraper.save_portal_texts(portal, texts, f'{portal}_political_news.txt')
```
#### MÃ©todos DisponÃ­veis
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `scrape_g1(limit_per_columnist=100)` | Coleta textos dos colunistas polÃ­ticos do G1 |
| `scrape_cnn(limit_pages=10)` | Coleta textos dos colunistas da CNN Brasil |
| `scrape_folha(limit_per_columnist=100)` | Coleta textos dos colunistas da Folha |
| `scrape_gazeta(limit_per_columnist=20)` | Coleta textos dos colunistas da Gazeta |
| `scrape_istoe(limit_per_columnist=100)` | Coleta textos dos colunistas da IstoÃ‰ |
| `scrape_metropoles(limit_per_columnist=20)` | Coleta textos dos colunistas do MetrÃ³poles |
| `scrape_all(limit_per_columnist=100)` | Coleta textos de todos os portais |
| `save_portal_texts(portal, texts, filename)` | Salva os textos coletados em arquivo |

#### Exemplo de Coleta EspecÃ­fica
```python
# Coleta apenas da CNN Brasil
cnn_texts = scraper.scrape_cnn(limit_pages=10)
scraper.save_portal_texts('cnn', cnn_texts, 'cnn_political_news.txt')

# Coleta apenas da Folha com limite personalizado
folha_texts = scraper.scrape_folha(limit_per_columnist=50)
scraper.save_portal_texts('folha', folha_texts, 'folha_political_news.txt')
```
#### ğŸ“‹ Requisitos
```bash
Python 3.7+
requests>=2.31.0
beautifulsoup4>=4.12.2
tqdm>=4.66.1
lxml>=4.9.3
```
### âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes
- O scraper respeita delays entre requisiÃ§Ãµes para evitar sobrecarga dos servidores
- Alguns portais podem requerer autenticaÃ§Ã£o para acesso completo ao conteÃºdo
- As classes CSS dos portais podem mudar, necessitando atualizaÃ§Ã£o do cÃ³digo
- O nÃºmero real de textos coletados pode ser menor que o limite definido

## ğŸ›ï¸ Coletor de Discursos de Deputados

### Sobre
Classe Python para coletar discursos de deputados atravÃ©s da API da CÃ¢mara dos Deputados e processo de enriquecimento com dados de espectro polÃ­tico dos partidos brasileiros

### ğŸš€ Como Usar

#### Uso BÃ¡sico
```python
from DiscursosDeputadosCollector import DiscursosDeputadosCollector
from PoliticalSpectrumEnricher import PoliticalSpectrumEnricher

# Exemplo de uso do coletor de discursos
collector = DiscursosDeputadosCollector()

path = '../../data/speech'
speech_file = f'{path}/Discursos.csv'
party_file = f'{path}/Partidos.csv'
merged_file = f'{path}/Discursos_Enriquecidos.csv'

# Coleta discursos de um perÃ­odo especÃ­fico
df = collector.collect_discursos(
    data_inicio='2025-02-01',
    data_fim='2025-02-05',
    output_file=speech_file
)

# Exemplo de uso do enriquecedor
enricher = PoliticalSpectrumEnricher()

# Carrega os dados
enricher.load_data(
    partidos_path=party_file,
    discursos_path=speech_file
)

# Enriquece os dados
df_enriched = enricher.enrich_data()

# Salva os dados enriquecidos
enricher.save_enriched_data(merged_file)
```
### ğŸ“Š Estrutura dos Dados Coletados e Enriquecidos

| Coluna | DescriÃ§Ã£o |
|:-------|:----------|
| `email` | Email institucional do deputado |
| `id` | ID Ãºnico do deputado na CÃ¢mara |
| `idLegislatura` | ID da legislatura atual |
| `nome` | Nome completo do parlamentar |
| `siglaPartido` | Sigla do partido polÃ­tico |
| `siglaUf` | Unidade federativa que representa |
| `uri` | URI do deputado na API |
| `uriPartido` | URI do partido na API |
| `urlFoto` | URL da foto oficial |
| `dataHoraFim` | Timestamp do fim do discurso |
| `dataHoraInicio` | Timestamp do inÃ­cio do discurso |
| `keywords` | Palavras-chave do discurso |
| `sumario` | Resumo do conteÃºdo |
| `tipoDiscurso` | ClassificaÃ§Ã£o do discurso |
| `transcricao` | Texto completo |
| `urlAudio` | Link para o Ã¡udio |
| `urlVideo` | Link para o vÃ­deo |
| `Espectro PolÃ­tico` | Espectro PolÃ­tico do Partido |

### ğŸ“‹ Requisitos
```bash
Python 3.7+
requests>=2.31.0
pandas>=2.0.0
python-dateutil>=2.8.2
```
### âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes
- A API pode ter limites de requisiÃ§Ãµes
- Alguns discursos podem nÃ£o ter transcriÃ§Ã£o disponÃ­vel
- O tempo de coleta pode variar dependendo do perÃ­odo solicitado
- Necessita conexÃ£o estÃ¡vel com a internet
<br><br>
# ğŸ¯ Treinamento e InferÃªncia do Modelo de ViÃ©s PolÃ­tico em MÃ­dia
## ğŸ“Š MediaBiasAnalyzer

### DescriÃ§Ã£o
Classe principal responsÃ¡vel por coordenar o processo de treinamento e anÃ¡lise de viÃ©s polÃ­tico em textos jornalÃ­sticos.

### Uso
```python
from MediaBiasAnalyzer import MediaBiasAnalyzer

path = '../../data/speech'
discursos = f'{path}/Discursos_Enriquecidos.csv'

# Inicializa o analisador
analyzer = MediaBiasAnalyzer()

# Treina o modelo
analyzer.train_model(training_data=discursos)

# Analisa todos os portais configurados
analyzer.analyze_media()
```
### âš ï¸ Notas Importantes
- O modelo BERT requer GPU para treinamento eficiente
- Textos muito longos sÃ£o truncados em 512 tokens
- Recomenda-se pelo menos 1000 exemplos para treinamento
- Os resultados podem variar dependendo dos dados de treinamento

## ğŸ“Š MediaBiasVisualizer

### DescriÃ§Ã£o
Classe responsÃ¡vel pela visualizaÃ§Ã£o e anÃ¡lise grÃ¡fica dos resultados de classificaÃ§Ã£o de viÃ©s polÃ­tico em textos jornalÃ­sticos.

### Uso BÃ¡sico
```python
from MediaBiasVisualizer import MediaBiasVisualizer

# Inicializa o visualizador
visualizer = MediaBiasVisualizer()

# Plota grÃ¡fico para um portal especÃ­fico
visualizer.plot_portal_bias('G1')
```
<br>

# ğŸ—ï¸ Estrutura do Projeto
```tree
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ portals
â”‚   â”‚   â”œâ”€â”€ cnn_political_news.txt
â”‚   â”‚   â”œâ”€â”€ folha_political_news.txt
â”‚   â”‚   â”œâ”€â”€ g1_political_news.txt
â”‚   â”‚   â”œâ”€â”€ gazeta_political_news.txt
â”‚   â”‚   â”œâ”€â”€ istoe_political_news.txt
â”‚   â”‚   â””â”€â”€ metropoles_political_news.txt
â”‚   â””â”€â”€ speech
â”‚       â””â”€â”€ Partidos.csv
â”œâ”€â”€ models
â”‚   â””â”€â”€ political_bias_model.joblib
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ MediaBiasReport.ipynb
â”œâ”€â”€ output
â”‚   â”œâ”€â”€ CNN_analysis.txt
â”‚   â”œâ”€â”€ CNN_predictions.csv
â”‚   â”œâ”€â”€ Folha_analysis.txt
â”‚   â”œâ”€â”€ Folha_predictions.csv
â”‚   â”œâ”€â”€ G1_analysis.txt
â”‚   â”œâ”€â”€ G1_predictions.csv
â”‚   â”œâ”€â”€ Gazeta_analysis.txt
â”‚   â”œâ”€â”€ Gazeta_predictions.csv
â”‚   â”œâ”€â”€ Istoe_analysis.txt
â”‚   â”œâ”€â”€ Istoe_predictions.csv
â”‚   â”œâ”€â”€ Metropoles_analysis.txt
â”‚   â”œâ”€â”€ Metropoles_predictions.csv
â”‚   â”œâ”€â”€ metrics_20250222_174248.txt
â”‚   â””â”€â”€ metrics_20250223_103248.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â”œâ”€â”€ main.py
    â”œâ”€â”€ model
    â”‚   â”œâ”€â”€ MediaBiasAnalyzer.py
    â”‚   â”œâ”€â”€ PoliticalBiasInferencer.py
    â”‚   â”œâ”€â”€ PoliticalBiasModelTrainer.py
    â”‚   â””â”€â”€ main.py
    â”œâ”€â”€ scrapper
    â”‚   â”œâ”€â”€ NewsPortalScraper.py
    â”‚   â”œâ”€â”€ NewsScraper.py
    â”‚   â””â”€â”€ main.py
    â”œâ”€â”€ speech
    â”‚   â”œâ”€â”€ DiscursosDeputadosCollector.py
    â”‚   â”œâ”€â”€ PoliticalSpectrumEnricher.py
    â”‚   â””â”€â”€ main.py
    â””â”€â”€ visual
        â””â”€â”€ MediaBiasVisualizer.py
```
<br>

# ğŸ“ LicenÃ§a
Este projeto estÃ¡ licenciado sob a licenÃ§a Apache-2.0 license - veja o arquivo [LICENSE](http://www.apache.org/licenses/LICENSE-2.0) para detalhes.

<br>

# ğŸ¤ Contribuindo
ContribuiÃ§Ãµes sÃ£o bem-vindas!