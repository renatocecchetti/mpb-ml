# mpb-ml - Media Political Bias Machine Learning
Este reposit√≥rio cont√©m o Modelo de Aprendizado de M√°quina, Conjunto de Dados e C√≥digo de Treinamento para An√°lise de Vi√©s Pol√≠tico em Ve√≠culos de Comunica√ß√£o Brasileiros

Pode ser utilizado de forma completa atrav√©s da execu√ß√£o do pipeline ou de forma modularizada realizando a chamada √† cada uma das fun√ß√µes. 
<br><br>
# üöÄ Instala√ß√£o
## Clone o reposit√≥rio
```bash
git clone https://github.com/renatocecchetti/mpb-ml.git
cd mpb-ml
```

## Instale as depend√™ncias
```bash
pip install -r requirements.txt
```
<br><br>

# Pipeline Completo
O pipeline ir√° realizar os seguintes passos:
1. Coleta dos discursos dos deputados
2. Enriquecimento com dados do espectro pol√≠tico dos partidos
3. Coleta de textos das colunas dos portais
4. Treinamento do modelo de classifica√ß√£o de vi√©s
5. Realiza√ß√£o de infer√™ncia nos dados dos portais

## üíª Como Usar
```bash
python src/main.py
```
## Visualiza√ß√£o dos Resultados
Um exemplo de visualiza√ß√£o do resultado do Pipeline Completo encontra-se dispon√≠vel no Jupyter Notebook [MediaBiasReport.ipynb](https://github.com/renatocecchetti/mpb-ml/blob/main/notebooks/MediaBiasReport.ipynb)

# Arquivo de Configura√ß√£o (config.yaml)
## Vis√£o Geral
O arquivo config.yaml centraliza todas as configura√ß√µes do sistema de an√°lise de vi√©s pol√≠tico em texto. Este guia detalha cada se√ß√£o e suas op√ß√µes.

Estrutura do Arquivo
1. Configura√ß√µes Gerais (general)
```bash
general:
  data_dir: 'data'              # Diret√≥rio base para dados
  data_dir_portals: 'data/portals'  # Subdiret√≥rio para dados dos portais
  data_dir_speech: 'data/speech'    # Subdiret√≥rio para discursos
  models_dir: 'models'          # Diret√≥rio para modelos treinados
  output_dir: 'output'          # Diret√≥rio para resultados
  log_level: 'INFO'             # N√≠vel de logging
  log_format: '%(asctime)s...'  # Formato das mensagens de log

```
2. Configura√ß√µes do Modelo (model)
```bash
model:
  name: 'political_bias_model.joblib'  # Nome do arquivo do modelo
  bert_model: 'neuralmind/bert-base-portuguese-cased'  # Modelo BERT pr√©-treinado
  hidden_layer_sizes: [100]     # Arquitetura da rede neural
  max_iter: 5000                # M√°ximo de itera√ß√µes
  random_state: 1               # Semente aleat√≥ria
  embeddings_file_name: 'embeddings.npy'  # Arquivo de embeddings
  reuse_embedding: False        # Reutilizar embeddings existentes
```
3. Portais de Not√≠cias (news_portals)
- supported_portals: Lista de portais suportados
- Para cada portal:
    - columnists: Dicion√°rio de colunistas e suas URLs
    - content_class: Classe CSS para extrair conte√∫do
    - post_class: Classe CSS para identificar posts

Exemplo de configura√ß√£o de portal:
```bash
g1:
  columnists:
    andreia_sadi: 'https://g1.globo.com/politica/blog/andreia-sadi/'
    # ...
  content_class: 'mc-column content-text active-extra-styles'
  post_class: 'bastian-feed-item'
```
4. Configura√ß√µes de Scraping (scraping)
```bash
scraping:
  user_agent: 'Mozilla/5.0...'  # User agent para requisi√ß√µes
  timeout: 10                   # Timeout em segundos
  sleep_time: 0.5              # Intervalo entre requisi√ß√µes
  max_retries: 3               # M√°ximo de tentativas
  items_per_page: 100          # Itens por p√°gina
  limit_per_columnist: 100     # Limite de artigos por colunista
```
5. Visualiza√ß√£o (visualization)
```bash
visualization:
  figure_size: [10, 6]         # Tamanho dos gr√°ficos
  colors:                      # Cores por orienta√ß√£o pol√≠tica
    left: 'red'
    center: 'gray'
    right: 'blue'
  spectrum_order: ['Esquerda', 'Centro', 'Direita']  # Ordem no gr√°fico
```
6. API da C√¢mara (camara_api)
```bash
camara_api:
  base_url: 'https://dadosabertos.camara.leg.br/api/v2'
  endpoints:                    # Endpoints da API
    deputados: '/deputados'
    discursos: '/deputados/{id}/discursos'
  params:                      # Par√¢metros padr√£o
    ordem: 'ASC'
    ordenarPor: 'nome'
    itens_por_pagina: 100
```
7. Configura√ß√µes de Discursos (discursos)
```bash
discursos:
  paths:                       # Caminhos dos arquivos
    base_dir: 'data/speech'
    discursos_file: 'Discursos.csv'
    # ...
  data_collection:            # Par√¢metros de coleta
    data_inicio: '2021-03-02'
    data_fim: '2025-03-01'
    sample_size: 5
  required_columns:           # Colunas obrigat√≥rias
    discursos: ['transcricao', 'siglaPartido', ...]
    partidos: ['Sigla', 'Nome', ...]
  spectrum_mapping:          # Mapeamento do espectro pol√≠tico
    'Centro': 'Centro'
    'Centro-direita': 'Direita'
    # ...
```
### Uso e Manuten√ß√£o
#### Adicionando Novos Portais
1. Adicione o nome do portal em supported_portals
2. Crie uma nova se√ß√£o com:
    - Lista de colunistas
    - Classes CSS necess√°rias
    - Configura√ß√µes espec√≠ficas

#### Atualizando Configura√ß√µes
- Scraping: Ajuste sleep_time e timeout conforme necess√°rio
- Modelo: Modifique par√¢metros do modelo em model
- Visualiza√ß√£o: Personalize cores e tamanhos em visualization

#### Manuten√ß√£o
- Verifique URLs periodicamente
- Atualize classes CSS quando os sites mudarem
- Ajuste par√¢metros de scraping conforme necessidade

#### Boas Pr√°ticas
- Mantenha backup do arquivo
- Documente altera√ß√µes
- Teste novas configura√ß√µes em ambiente de desenvolvimento
- Monitore logs para ajustes de par√¢metros

#### Observa√ß√µes
- Classes CSS podem mudar com atualiza√ß√µes dos portais
- Respeite limites de requisi√ß√µes dos sites
- Mantenha sleep_time adequado para evitar bloqueios
- Fa√ßa backup regular dos dados coletados

<br>
Este arquivo de configura√ß√£o centraliza todas as configura√ß√µes do sistema, facilitando manuten√ß√£o e ajustes sem necessidade de alterar o c√≥digo-fonte.

<br>

# Modularizado
## Scrapper de portais de not√≠cias
Sistema de coleta automatizada de not√≠cias dos principais portais jornal√≠sticos do Brasil.

### üì∞ Sobre
Sistema que realiza scraping de colunas pol√≠ticas dos seguintes portais:

- G1
- CNN Brasil
- Folha de S√£o Paulo
- Gazeta do Povo
- Isto√â
- Metr√≥poles

### üíª Como Usar
#### Uso B√°sico
```python
from NewsPortalScraper import NewsPortalScraper

# Inicializa o scraper
scraper = NewsPortalScraper()

# Coleta de um portal espec√≠fico
g1_texts = scraper.scrape_g1(limit_per_columnist=100)
scraper.save_portal_texts('g1', g1_texts, 'g1_political_news.txt')

# Ou coleta de todos os portais
all_texts = scraper.scrape_all(limit_per_columnist=100)
for portal, texts in all_texts.items():
    scraper.save_portal_texts(portal, texts, f'{portal}_political_news.txt')
```
#### M√©todos Dispon√≠veis
| M√©todo | Descri√ß√£o |
|--------|-----------|
| `scrape_g1(limit_per_columnist=100)` | Coleta textos dos colunistas pol√≠ticos do G1 |
| `scrape_cnn(limit_pages=10)` | Coleta textos dos colunistas da CNN Brasil |
| `scrape_folha(limit_per_columnist=100)` | Coleta textos dos colunistas da Folha |
| `scrape_gazeta(limit_per_columnist=20)` | Coleta textos dos colunistas da Gazeta |
| `scrape_istoe(limit_per_columnist=100)` | Coleta textos dos colunistas da Isto√â |
| `scrape_metropoles(limit_per_columnist=20)` | Coleta textos dos colunistas do Metr√≥poles |
| `scrape_all(limit_per_columnist=100)` | Coleta textos de todos os portais |
| `save_portal_texts(portal, texts, filename)` | Salva os textos coletados em arquivo |

#### Exemplo de Coleta Espec√≠fica
```python
# Coleta apenas da CNN Brasil
cnn_texts = scraper.scrape_cnn(limit_pages=10)
scraper.save_portal_texts('cnn', cnn_texts, 'cnn_political_news.txt')

# Coleta apenas da Folha com limite personalizado
folha_texts = scraper.scrape_folha(limit_per_columnist=50)
scraper.save_portal_texts('folha', folha_texts, 'folha_political_news.txt')
```
#### üìã Requisitos
```bash
Python 3.7+
requests>=2.31.0
beautifulsoup4>=4.12.2
tqdm>=4.66.1
lxml>=4.9.3
```
### ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes
- O scraper respeita delays entre requisi√ß√µes para evitar sobrecarga dos servidores
- Alguns portais podem requerer autentica√ß√£o para acesso completo ao conte√∫do
- As classes CSS dos portais podem mudar, necessitando atualiza√ß√£o do c√≥digo
- O n√∫mero real de textos coletados pode ser menor que o limite definido

## üèõÔ∏è Coletor de Discursos de Deputados

### Sobre
Classe Python para coletar discursos de deputados atrav√©s da API da C√¢mara dos Deputados e processo de enriquecimento com dados de espectro pol√≠tico dos partidos brasileiros

### üöÄ Como Usar

#### Uso B√°sico
```python
from DiscursosDeputadosCollector import DiscursosDeputadosCollector
from PoliticalSpectrumEnricher import PoliticalSpectrumEnricher

# Exemplo de uso do coletor de discursos
collector = DiscursosDeputadosCollector()

path = '../../data/speech'
speech_file = f'{path}/Discursos.csv'
party_file = f'{path}/Partidos.csv'
merged_file = f'{path}/Discursos_Enriquecidos.csv'

# Coleta discursos de um per√≠odo espec√≠fico
df = collector.collect_discursos(
    data_inicio='2025-02-01',
    data_fim='2025-02-05',
    output_file=speech_file
)

# Exemplo de uso do enriquecedor
enricher = PoliticalSpectrumEnricher()

# Carrega os dados
enricher.load_data(
    partidos_path=party_file,
    discursos_path=speech_file
)

# Enriquece os dados
df_enriched = enricher.enrich_data()

# Salva os dados enriquecidos
enricher.save_enriched_data(merged_file)
```
### üìä Estrutura dos Dados Coletados e Enriquecidos

| Coluna | Descri√ß√£o |
|:-------|:----------|
| `email` | Email institucional do deputado |
| `id` | ID √∫nico do deputado na C√¢mara |
| `idLegislatura` | ID da legislatura atual |
| `nome` | Nome completo do parlamentar |
| `siglaPartido` | Sigla do partido pol√≠tico |
| `siglaUf` | Unidade federativa que representa |
| `uri` | URI do deputado na API |
| `uriPartido` | URI do partido na API |
| `urlFoto` | URL da foto oficial |
| `dataHoraFim` | Timestamp do fim do discurso |
| `dataHoraInicio` | Timestamp do in√≠cio do discurso |
| `keywords` | Palavras-chave do discurso |
| `sumario` | Resumo do conte√∫do |
| `tipoDiscurso` | Classifica√ß√£o do discurso |
| `transcricao` | Texto completo |
| `urlAudio` | Link para o √°udio |
| `urlVideo` | Link para o v√≠deo |
| `Espectro Pol√≠tico` | Espectro Pol√≠tico do Partido |

### üìã Requisitos
```bash
Python 3.7+
requests>=2.31.0
pandas>=2.0.0
python-dateutil>=2.8.2
```
### ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes
- A API pode ter limites de requisi√ß√µes
- Alguns discursos podem n√£o ter transcri√ß√£o dispon√≠vel
- O tempo de coleta pode variar dependendo do per√≠odo solicitado
- Necessita conex√£o est√°vel com a internet
<br><br>
# üéØ Treinamento e Infer√™ncia do Modelo de Vi√©s Pol√≠tico em M√≠dia
## üìä MediaBiasAnalyzer

### Descri√ß√£o
Classe principal respons√°vel por coordenar o processo de treinamento e an√°lise de vi√©s pol√≠tico em textos jornal√≠sticos.

### Uso
```python
from MediaBiasAnalyzer import MediaBiasAnalyzer

path = '../../data/speech'
discursos = f'{path}/Discursos_Enriquecidos.csv'

# Inicializa o analisador
analyzer = MediaBiasAnalyzer()

# Treina o modelo
analyzer.train_model(training_data=discursos)

# Analisa todos os portais configurados
analyzer.analyze_media()
```
### ‚ö†Ô∏è Notas Importantes
- O modelo BERT requer GPU para treinamento eficiente
- Textos muito longos s√£o truncados em 512 tokens
- Recomenda-se pelo menos 1000 exemplos para treinamento
- Os resultados podem variar dependendo dos dados de treinamento

## üìä MediaBiasVisualizer

### Descri√ß√£o
Classe respons√°vel pela visualiza√ß√£o e an√°lise gr√°fica dos resultados de classifica√ß√£o de vi√©s pol√≠tico em textos jornal√≠sticos.

### Uso B√°sico
```python
from MediaBiasVisualizer import MediaBiasVisualizer

# Inicializa o visualizador
visualizer = MediaBiasVisualizer()

# Plota gr√°fico para um portal espec√≠fico
visualizer.plot_portal_bias('G1')
```
<br>

# üèóÔ∏è Estrutura do Projeto
```tree
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ portals
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cnn_political_news.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ folha_political_news.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ g1_political_news.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gazeta_political_news.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ istoe_political_news.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metropoles_political_news.txt
‚îÇ   ‚îî‚îÄ‚îÄ speech
‚îÇ       ‚îî‚îÄ‚îÄ Partidos.csv
‚îú‚îÄ‚îÄ models
‚îÇ   ‚îî‚îÄ‚îÄ political_bias_model.joblib
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ MediaBiasReport.ipynb
‚îú‚îÄ‚îÄ output
‚îÇ   ‚îú‚îÄ‚îÄ CNN_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ CNN_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ Folha_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ Folha_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ G1_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ G1_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ Gazeta_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ Gazeta_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ Istoe_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ Istoe_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ Metropoles_analysis.txt
‚îÇ   ‚îú‚îÄ‚îÄ Metropoles_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ metrics_20250222_174248.txt
‚îÇ   ‚îî‚îÄ‚îÄ metrics_20250223_103248.txt
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ src
    ‚îú‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ model
    ‚îÇ   ‚îú‚îÄ‚îÄ MediaBiasAnalyzer.py
    ‚îÇ   ‚îú‚îÄ‚îÄ PoliticalBiasInferencer.py
    ‚îÇ   ‚îú‚îÄ‚îÄ PoliticalBiasModelTrainer.py
    ‚îÇ   ‚îî‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ scrapper
    ‚îÇ   ‚îú‚îÄ‚îÄ NewsPortalScraper.py
    ‚îÇ   ‚îú‚îÄ‚îÄ NewsScraper.py
    ‚îÇ   ‚îî‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ speech
    ‚îÇ   ‚îú‚îÄ‚îÄ DiscursosDeputadosCollector.py
    ‚îÇ   ‚îú‚îÄ‚îÄ PoliticalSpectrumEnricher.py
    ‚îÇ   ‚îî‚îÄ‚îÄ main.py
    ‚îî‚îÄ‚îÄ visual
        ‚îî‚îÄ‚îÄ MediaBiasVisualizer.py
```
<br>

# üìù Licen√ßa
Este projeto est√° licenciado sob a licen√ßa Apache-2.0 license - veja o arquivo [LICENSE](http://www.apache.org/licenses/LICENSE-2.0) para detalhes.

<br>

# ü§ù Contribuindo
Contribui√ß√µes s√£o bem-vindas!